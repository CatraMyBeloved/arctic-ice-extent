{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multivariate LSTM for Arctic Sea Ice Extent Forecasting\n",
    "\n",
    "This notebook extends the basic LSTM by incorporating pan-arctic climate variables as additional features."
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:14.545224Z",
     "start_time": "2025-10-13T13:29:12.405282Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data_utils import load_data"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "dataset_class",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:14.556245Z",
     "start_time": "2025-10-13T13:29:14.549237Z"
    }
   },
   "source": "class MultivariateArcticDataset(torch.utils.data.Dataset):\n\n    def __init__(self, data, sequence_length=30, forecast_horizon=1, features=None, target='extent_mkm2', scaler=None, lag_features=None, add_cyclical_time=False):\n        self.data = data.sort_values('date').reset_index(drop=True)\n        self.sequence_length = sequence_length\n        self.forecast_horizon = forecast_horizon\n        self.target = target\n        \n        if features is None:\n            self.features = ['extent_mkm2']\n        else:\n            self.features = features.copy()\n        \n        if add_cyclical_time:\n            day_of_year = pd.to_datetime(self.data['date']).dt.dayofyear\n            self.data['day_of_year_sin'] = np.sin(2 * np.pi * day_of_year / 365.25)\n            self.data['day_of_year_cos'] = np.cos(2 * np.pi * day_of_year / 365.25)\n            self.features.extend(['day_of_year_sin', 'day_of_year_cos'])\n        \n        if lag_features is not None:\n            for column, lags in lag_features.items():\n                for lag_days in lags:\n                    lagged_column_name = f\"{column}_lag{lag_days}\"\n                    self.data[lagged_column_name] = self.data[column].shift(lag_days)\n                    self.features.append(lagged_column_name)\n            \n            self.data = self.data.dropna().reset_index(drop=True)\n        \n        self.data_values = self.data[self.features].values.astype(np.float32)\n        \n        self.target_idx = self.features.index(self.target)\n        \n        if scaler is None:\n            self.mean = self.data_values.mean(axis=0, keepdims=True)\n            self.std = self.data_values.std(axis=0, keepdims=True)\n            self.std = np.where(self.std == 0, 1.0, self.std)\n        else:\n            self.mean, self.std = scaler\n        \n        self.data_normalized = (self.data_values - self.mean) / self.std\n    \n    def __len__(self):\n        return len(self.data_normalized) - self.sequence_length - self.forecast_horizon + 1\n    \n    def __getitem__(self, idx):\n        X = self.data_normalized[idx:idx + self.sequence_length]\n        \n        y = self.data_normalized[idx + self.sequence_length + self.forecast_horizon - 1][self.target_idx]\n        \n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.float32)\n        \n        return X, y",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "data_loading_header",
   "metadata": {},
   "source": [
    "## Load Data and Explore Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "load_data",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:19.228315Z",
     "start_time": "2025-10-13T13:29:14.562491Z"
    }
   },
   "source": [
    "# Load training data (1989-2019)\n",
    "train_data = load_data(regions='pan_arctic', years=range(1989, 2020))\n",
    "\n",
    "# Load test data (2020-2023)\n",
    "test_data = load_data(regions='pan_arctic', years=range(2020, 2024))\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"\\nAvailable columns:\")\n",
    "for col in train_data.columns:\n",
    "    print(f\"  - {col}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (11322, 27)\n",
      "Test data shape: (1461, 27)\n",
      "\n",
      "Available columns:\n",
      "  - date\n",
      "  - region\n",
      "  - msl_mean\n",
      "  - msl_p15\n",
      "  - msl_p85\n",
      "  - msl_std\n",
      "  - t2m_mean\n",
      "  - t2m_p15\n",
      "  - t2m_p85\n",
      "  - t2m_std\n",
      "  - tp_mean\n",
      "  - tp_p15\n",
      "  - tp_p85\n",
      "  - tp_std\n",
      "  - u10_mean\n",
      "  - u10_p15\n",
      "  - u10_p85\n",
      "  - u10_std\n",
      "  - v10_mean\n",
      "  - v10_p15\n",
      "  - v10_p85\n",
      "  - v10_std\n",
      "  - wind_speed_mean\n",
      "  - wind_speed_p15\n",
      "  - wind_speed_p85\n",
      "  - wind_speed_std\n",
      "  - extent_mkm2\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9rinw3xpxim",
   "source": [
    "features = [\n",
    "    'extent_mkm2',\n",
    "    't2m_mean',\n",
    "    't2m_std',\n",
    "    'msl_mean',\n",
    "    'msl_std',\n",
    "    'wind_speed_mean',\n",
    "    'wind_speed_std',\n",
    "]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:19.322143Z",
     "start_time": "2025-10-13T13:29:19.318977Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "tk3joma1ece",
   "source": "## Multivariate LSTM Model\n\nLSTM cells naturally handle multivariate sequences because they process each timestep's feature vector through the same weight matrices.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jue5nlflp5q",
   "source": "class IceExtentLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n        super(IceExtentLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        \n        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n        \n        out = self.fc(out[:, -1, :])\n        \n        return out",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:19.341039Z",
     "start_time": "2025-10-13T13:29:19.325180Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "h8idz2sfz9i",
   "source": "## Create Datasets (Non-Lagged)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "12mv7qny7xvk",
   "source": "train_dataset = MultivariateArcticDataset(\n    train_data, \n    sequence_length=30, \n    forecast_horizon=1, \n    features=features,\n    target='extent_mkm2'\n)\n\ntest_dataset = MultivariateArcticDataset(\n    test_data,\n    sequence_length=30,\n    forecast_horizon=1,\n    features=features,\n    target='extent_mkm2',\n    scaler=(train_dataset.mean, train_dataset.std)\n)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\nfor X_batch, y_batch in train_loader:\n    print(f\"\\nBatch X shape: {X_batch.shape}\")\n    print(f\"Batch y shape: {y_batch.shape}\")\n    break",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:19.357181Z",
     "start_time": "2025-10-13T13:29:19.347701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11292\n",
      "Test samples: 1431\n",
      "\n",
      "Batch X shape: torch.Size([32, 30, 7])\n",
      "Batch y shape: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "disff4kwj3i",
   "source": "## Train Model (Non-Lagged)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "r7swl5uhg7d",
   "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel = IceExtentLSTM(input_size=len(features), hidden_size=64, num_layers=2, output_size=1, dropout=0.2)\nmodel = model.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:29:21.035693Z",
     "start_time": "2025-10-13T13:29:19.365317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model parameters: 52,033\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "psazpxll23",
   "source": "num_epochs = 200\nbest_val_loss = float('inf')\npatience = 15\npatience_counter = 0\n\ntrain_losses = []\nval_losses = []\n\nprint(\"Starting training...\\n\")\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for X_batch, y_batch in train_loader:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        predictions = model(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n\n    scheduler.step(avg_val_loss)\n\n    if (epoch + 1) % 5 == 0:\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {avg_train_loss:.6f}')\n        print(f'  Val Loss: {avg_val_loss:.6f}')\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), 'best_multivariate_model.pt')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:33:33.018725Z",
     "start_time": "2025-10-13T13:29:21.068830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 5/200\n",
      "  Train Loss: 0.002130\n",
      "  Val Loss: 0.001298\n",
      "Epoch 10/200\n",
      "  Train Loss: 0.001572\n",
      "  Val Loss: 0.001344\n",
      "Epoch 15/200\n",
      "  Train Loss: 0.001221\n",
      "  Val Loss: 0.001461\n",
      "Epoch 20/200\n",
      "  Train Loss: 0.001112\n",
      "  Val Loss: 0.000868\n",
      "Epoch 25/200\n",
      "  Train Loss: 0.000983\n",
      "  Val Loss: 0.000851\n",
      "Epoch 30/200\n",
      "  Train Loss: 0.000938\n",
      "  Val Loss: 0.001075\n",
      "Epoch 35/200\n",
      "  Train Loss: 0.000852\n",
      "  Val Loss: 0.000688\n",
      "Epoch 40/200\n",
      "  Train Loss: 0.000779\n",
      "  Val Loss: 0.000665\n",
      "Epoch 45/200\n",
      "  Train Loss: 0.000703\n",
      "  Val Loss: 0.000447\n",
      "Epoch 50/200\n",
      "  Train Loss: 0.000661\n",
      "  Val Loss: 0.000419\n",
      "Epoch 55/200\n",
      "  Train Loss: 0.000578\n",
      "  Val Loss: 0.000502\n",
      "Epoch 60/200\n",
      "  Train Loss: 0.000497\n",
      "  Val Loss: 0.000504\n",
      "Epoch 65/200\n",
      "  Train Loss: 0.000489\n",
      "  Val Loss: 0.000400\n",
      "Epoch 70/200\n",
      "  Train Loss: 0.000430\n",
      "  Val Loss: 0.000406\n",
      "Epoch 75/200\n",
      "  Train Loss: 0.000425\n",
      "  Val Loss: 0.000396\n",
      "Epoch 80/200\n",
      "  Train Loss: 0.000399\n",
      "  Val Loss: 0.000350\n",
      "Epoch 85/200\n",
      "  Train Loss: 0.000386\n",
      "  Val Loss: 0.000342\n",
      "Epoch 90/200\n",
      "  Train Loss: 0.000387\n",
      "  Val Loss: 0.000344\n",
      "Epoch 95/200\n",
      "  Train Loss: 0.000384\n",
      "  Val Loss: 0.000332\n",
      "Epoch 100/200\n",
      "  Train Loss: 0.000378\n",
      "  Val Loss: 0.000323\n",
      "Epoch 105/200\n",
      "  Train Loss: 0.000372\n",
      "  Val Loss: 0.000324\n",
      "Epoch 110/200\n",
      "  Train Loss: 0.000369\n",
      "  Val Loss: 0.000348\n",
      "Epoch 115/200\n",
      "  Train Loss: 0.000372\n",
      "  Val Loss: 0.000337\n",
      "\n",
      "Early stopping at epoch 115\n",
      "\n",
      "Training complete! Best validation loss: 0.000323\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "11yj5mv8obub",
   "source": "## Lagged Features\n\nAdd temporal lags to capture recent trends in ice extent and temperature.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "85fe8tp3i9b",
   "source": "lag_features = {\n    'extent_mkm2': [7, 14, 30],\n    't2m_mean': [7, 14, 30],\n}",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:33:33.110020Z",
     "start_time": "2025-10-13T13:33:33.107380Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "oqh5y0g845",
   "source": "## Create Datasets with Lagged Features\n\nThe Dataset class automatically creates lagged columns and adds them to the feature set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x0miygw4dsa",
   "source": "train_dataset_lagged = MultivariateArcticDataset(\n    train_data, \n    sequence_length=30, \n    forecast_horizon=1,\n    features=features,\n    target='extent_mkm2',\n    lag_features=lag_features\n)\n\ntest_dataset_lagged = MultivariateArcticDataset(\n    test_data,\n    sequence_length=30,\n    forecast_horizon=1,\n    features=features,\n    target='extent_mkm2',\n    scaler=(train_dataset_lagged.mean, train_dataset_lagged.std),\n    lag_features=lag_features\n)\n\ntrain_loader_lagged = torch.utils.data.DataLoader(train_dataset_lagged, batch_size=32, shuffle=True)\ntest_loader_lagged = torch.utils.data.DataLoader(test_dataset_lagged, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset_lagged)}\")\nprint(f\"Test samples: {len(test_dataset_lagged)}\")\nprint(f\"Number of features: {len(train_dataset_lagged.features)}\")\nprint(f\"\\nFeatures:\")\nfor i, f in enumerate(train_dataset_lagged.features):\n    print(f\"  {i}: {f}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:33:33.153561Z",
     "start_time": "2025-10-13T13:33:33.123535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11262\n",
      "Test samples: 1401\n",
      "Number of features: 13\n",
      "\n",
      "Features:\n",
      "  0: extent_mkm2\n",
      "  1: t2m_mean\n",
      "  2: t2m_std\n",
      "  3: msl_mean\n",
      "  4: msl_std\n",
      "  5: wind_speed_mean\n",
      "  6: wind_speed_std\n",
      "  7: extent_mkm2_lag7\n",
      "  8: extent_mkm2_lag14\n",
      "  9: extent_mkm2_lag30\n",
      "  10: t2m_mean_lag7\n",
      "  11: t2m_mean_lag14\n",
      "  12: t2m_mean_lag30\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "24jv0k8gasz",
   "source": "for X_batch, y_batch in train_loader_lagged:\n    print(f\"Batch X shape: {X_batch.shape}\")\n    print(f\"Batch y shape: {y_batch.shape}\")\n    break",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:33:33.168422Z",
     "start_time": "2025-10-13T13:33:33.161425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([32, 30, 13])\n",
      "Batch y shape: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "6o3nyc1tqdj",
   "source": "## Train Model with Lagged Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tognjrbgecl",
   "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel_lagged = IceExtentLSTM(\n    input_size=len(train_dataset_lagged.features), \n    hidden_size=64, \n    num_layers=2, \n    output_size=1, \n    dropout=0.2\n)\nmodel_lagged = model_lagged.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model_lagged.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model_lagged.parameters()):,}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:33:33.192462Z",
     "start_time": "2025-10-13T13:33:33.185452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model parameters: 53,569\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8l16edplnqi",
   "source": "num_epochs = 200\nbest_val_loss = float('inf')\npatience = 15\npatience_counter = 0\n\ntrain_losses_lagged = []\nval_losses_lagged = []\n\nprint(\"Starting training...\\n\")\nfor epoch in range(num_epochs):\n    model_lagged.train()\n    train_loss = 0.0\n\n    for X_batch, y_batch in train_loader_lagged:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        predictions = model_lagged(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model_lagged.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader_lagged)\n\n    model_lagged.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader_lagged:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            predictions = model_lagged(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader_lagged)\n\n    train_losses_lagged.append(avg_train_loss)\n    val_losses_lagged.append(avg_val_loss)\n\n    scheduler.step(avg_val_loss)\n\n    if (epoch + 1) % 5 == 0:\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {avg_train_loss:.6f}')\n        print(f'  Val Loss: {avg_val_loss:.6f}')\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model_lagged.state_dict(), 'best_lagged_model.pt')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:37:54.231419Z",
     "start_time": "2025-10-13T13:33:33.202467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 5/200\n",
      "  Train Loss: 0.002292\n",
      "  Val Loss: 0.002560\n",
      "Epoch 10/200\n",
      "  Train Loss: 0.001585\n",
      "  Val Loss: 0.001721\n",
      "Epoch 15/200\n",
      "  Train Loss: 0.001330\n",
      "  Val Loss: 0.000811\n",
      "Epoch 20/200\n",
      "  Train Loss: 0.001038\n",
      "  Val Loss: 0.000773\n",
      "Epoch 25/200\n",
      "  Train Loss: 0.000939\n",
      "  Val Loss: 0.001622\n",
      "Epoch 30/200\n",
      "  Train Loss: 0.000908\n",
      "  Val Loss: 0.000952\n",
      "Epoch 35/200\n",
      "  Train Loss: 0.000741\n",
      "  Val Loss: 0.000665\n",
      "Epoch 40/200\n",
      "  Train Loss: 0.000698\n",
      "  Val Loss: 0.000914\n",
      "Epoch 45/200\n",
      "  Train Loss: 0.000633\n",
      "  Val Loss: 0.000693\n",
      "Epoch 50/200\n",
      "  Train Loss: 0.000697\n",
      "  Val Loss: 0.000641\n",
      "Epoch 55/200\n",
      "  Train Loss: 0.000501\n",
      "  Val Loss: 0.000447\n",
      "Epoch 60/200\n",
      "  Train Loss: 0.000498\n",
      "  Val Loss: 0.000542\n",
      "Epoch 65/200\n",
      "  Train Loss: 0.000471\n",
      "  Val Loss: 0.000364\n",
      "Epoch 70/200\n",
      "  Train Loss: 0.000424\n",
      "  Val Loss: 0.000365\n",
      "Epoch 75/200\n",
      "  Train Loss: 0.000421\n",
      "  Val Loss: 0.000361\n",
      "Epoch 80/200\n",
      "  Train Loss: 0.000411\n",
      "  Val Loss: 0.000393\n",
      "Epoch 85/200\n",
      "  Train Loss: 0.000376\n",
      "  Val Loss: 0.000313\n",
      "Epoch 90/200\n",
      "  Train Loss: 0.000377\n",
      "  Val Loss: 0.000316\n",
      "Epoch 95/200\n",
      "  Train Loss: 0.000358\n",
      "  Val Loss: 0.000318\n",
      "Epoch 100/200\n",
      "  Train Loss: 0.000348\n",
      "  Val Loss: 0.000311\n",
      "Epoch 105/200\n",
      "  Train Loss: 0.000344\n",
      "  Val Loss: 0.000315\n",
      "Epoch 110/200\n",
      "  Train Loss: 0.000345\n",
      "  Val Loss: 0.000311\n",
      "Epoch 115/200\n",
      "  Train Loss: 0.000337\n",
      "  Val Loss: 0.000307\n",
      "Epoch 120/200\n",
      "  Train Loss: 0.000339\n",
      "  Val Loss: 0.000311\n",
      "\n",
      "Early stopping at epoch 123\n",
      "\n",
      "Training complete! Best validation loss: 0.000306\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "f4pac1zmihh",
   "source": "## Cyclical Day-of-Year Encoding\n\nArctic ice extent has strong seasonal patterns. Cyclical encoding (sin/cos) preserves the circular nature of time where Dec 31 and Jan 1 are neighbors.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ms52ixg89sp",
   "source": "train_dataset_cyclical = MultivariateArcticDataset(\n    train_data, \n    sequence_length=30, \n    forecast_horizon=1,\n    features=features,\n    target='extent_mkm2',\n    add_cyclical_time=True\n)\n\ntest_dataset_cyclical = MultivariateArcticDataset(\n    test_data,\n    sequence_length=30,\n    forecast_horizon=1,\n    features=features,\n    target='extent_mkm2',\n    scaler=(train_dataset_cyclical.mean, train_dataset_cyclical.std),\n    add_cyclical_time=True\n)\n\ntrain_loader_cyclical = torch.utils.data.DataLoader(train_dataset_cyclical, batch_size=32, shuffle=True)\ntest_loader_cyclical = torch.utils.data.DataLoader(test_dataset_cyclical, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset_cyclical)}\")\nprint(f\"Test samples: {len(test_dataset_cyclical)}\")\nprint(f\"Number of features: {len(train_dataset_cyclical.features)}\")\nprint(f\"\\nFeatures:\")\nfor i, f in enumerate(train_dataset_cyclical.features):\n    print(f\"  {i}: {f}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:38:17.084828Z",
     "start_time": "2025-10-13T13:38:17.065369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11292\n",
      "Test samples: 1431\n",
      "Number of features: 9\n",
      "\n",
      "Features:\n",
      "  0: extent_mkm2\n",
      "  1: t2m_mean\n",
      "  2: t2m_std\n",
      "  3: msl_mean\n",
      "  4: msl_std\n",
      "  5: wind_speed_mean\n",
      "  6: wind_speed_std\n",
      "  7: day_of_year_sin\n",
      "  8: day_of_year_cos\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "vdfdpd54zhq",
   "source": "## Train Model with Cyclical Time Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ghtemlrug9m",
   "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel_cyclical = IceExtentLSTM(\n    input_size=len(train_dataset_cyclical.features), \n    hidden_size=64, \n    num_layers=2, \n    output_size=1, \n    dropout=0.2\n)\nmodel_cyclical = model_cyclical.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model_cyclical.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model_cyclical.parameters()):,}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:38:19.586633Z",
     "start_time": "2025-10-13T13:38:19.580121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model parameters: 52,545\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "avk0irua7th",
   "source": [
    "num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses_cyclical = []\n",
    "val_losses_cyclical = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_cyclical.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader_cyclical:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        predictions = model_cyclical(X_batch)\n",
    "        loss = criterion(predictions.squeeze(), y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_cyclical.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader_cyclical)\n",
    "\n",
    "    model_cyclical.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader_cyclical:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            predictions = model_cyclical(X_batch)\n",
    "            loss = criterion(predictions.squeeze(), y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader_cyclical)\n",
    "\n",
    "    train_losses_cyclical.append(avg_train_loss)\n",
    "    val_losses_cyclical.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {avg_train_loss:.6f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.6f}')\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model_cyclical.state_dict(), 'best_cyclical_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T13:42:07.826771Z",
     "start_time": "2025-10-13T13:38:23.014768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 5/200\n",
      "  Train Loss: 0.002004\n",
      "  Val Loss: 0.001188\n",
      "Epoch 10/200\n",
      "  Train Loss: 0.001784\n",
      "  Val Loss: 0.001101\n",
      "Epoch 15/200\n",
      "  Train Loss: 0.001332\n",
      "  Val Loss: 0.001086\n",
      "Epoch 20/200\n",
      "  Train Loss: 0.001158\n",
      "  Val Loss: 0.000900\n",
      "Epoch 25/200\n",
      "  Train Loss: 0.000848\n",
      "  Val Loss: 0.000647\n",
      "Epoch 30/200\n",
      "  Train Loss: 0.000827\n",
      "  Val Loss: 0.000807\n",
      "Epoch 35/200\n",
      "  Train Loss: 0.000663\n",
      "  Val Loss: 0.000672\n",
      "Epoch 40/200\n",
      "  Train Loss: 0.000619\n",
      "  Val Loss: 0.000508\n",
      "Epoch 45/200\n",
      "  Train Loss: 0.000582\n",
      "  Val Loss: 0.000520\n",
      "Epoch 50/200\n",
      "  Train Loss: 0.000559\n",
      "  Val Loss: 0.000421\n",
      "Epoch 55/200\n",
      "  Train Loss: 0.000525\n",
      "  Val Loss: 0.000424\n",
      "Epoch 60/200\n",
      "  Train Loss: 0.000511\n",
      "  Val Loss: 0.000360\n",
      "Epoch 65/200\n",
      "  Train Loss: 0.000512\n",
      "  Val Loss: 0.000404\n",
      "Epoch 70/200\n",
      "  Train Loss: 0.000442\n",
      "  Val Loss: 0.000398\n",
      "Epoch 75/200\n",
      "  Train Loss: 0.000436\n",
      "  Val Loss: 0.000346\n",
      "Epoch 80/200\n",
      "  Train Loss: 0.000407\n",
      "  Val Loss: 0.000328\n",
      "Epoch 85/200\n",
      "  Train Loss: 0.000397\n",
      "  Val Loss: 0.000339\n",
      "Epoch 90/200\n",
      "  Train Loss: 0.000391\n",
      "  Val Loss: 0.000336\n",
      "Epoch 95/200\n",
      "  Train Loss: 0.000392\n",
      "  Val Loss: 0.000338\n",
      "\n",
      "Early stopping at epoch 99\n",
      "\n",
      "Training complete! Best validation loss: 0.000321\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3902cfd295dae6db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
