{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "77c83102e976dfe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:54:53.448302Z",
     "start_time": "2025-08-28T10:54:51.921718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core scientific and data libraries\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from src.coordinate_utils import *\n",
    "import dask\n",
    "from dask.distributed import Client"
   ],
   "id": "799505498c75b301",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variables\n",
   "id": "ffa768fe592de812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:54:53.456690Z",
     "start_time": "2025-08-28T10:54:53.452693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_url = 'gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr'\n",
    "relevant_regions = ['Central_Arctic', 'Beaufort', 'Chukchi-NA', 'Chukchi-Asia', 'E_Siberian', 'Laptev', 'Kara', 'Barents', 'E_Greenland', 'Baffin', 'Hudson', 'Can_Arch', 'Bering-NA', 'Bering-Asia', 'pan_arctic']"
   ],
   "id": "97df7811dd489449",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:55:14.733715Z",
     "start_time": "2025-08-28T10:54:53.466091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = xr.open_zarr(dataset_url, chunks={'time': 720 , 'values': 'auto'},\n",
    "                       consolidated=True, decode_timedelta=False)\n",
    "dataset = dataset.assign_coords(longitude=((dataset.longitude + 180) % 360) - 180)\n"
   ],
   "id": "2e6b638e550628c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stein\\AppData\\Local\\Temp\\ipykernel_22904\\1045733319.py:1: UserWarning: The specified chunks separate the stored chunks along dimension \"values\" starting at index 46603. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  dataset = xr.open_zarr(dataset_url, chunks={'time': 720 , 'values': 'auto'},\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:55:14.762377Z",
     "start_time": "2025-08-28T10:55:14.758869Z"
    }
   },
   "cell_type": "code",
   "source": "selected_vars_dataset = dataset[[\"t2m\", \"u10\", \"v10\", \"msl\"]]",
   "id": "7f9cb90250ef6c6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper Functions",
   "id": "3681e242326c231c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:55:14.782862Z",
     "start_time": "2025-08-28T10:55:14.768290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_and_cache_mask(region_name):\n",
    "    \"\"\"\n",
    "    Creates and caches a mask for a specified geographical region.\n",
    "\n",
    "    This function generates a mask by determining whether geographical points\n",
    "    belong to a specified region shape, using longitude and latitude coordinates\n",
    "    from a dataset. The mask is then cached as a .npy file for future use.\n",
    "    It supports both \"pan_arctic\" and other custom region names.\n",
    "\n",
    "    Parameters:\n",
    "        region_name (str): Name of the geographical region for which the mask\n",
    "            is to be created. If \"pan_arctic\", a predefined shape is used, otherwise\n",
    "            a specified regional shape is retrieved.\n",
    "\n",
    "    Raises:\n",
    "        Any exceptions associated with file I/O or operations on the dataset.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A boolean array where each element indicates whether\n",
    "            the corresponding geographical point is within the specified region.\n",
    "    \"\"\"\n",
    "    coords_only = xr.open_zarr(dataset_url, chunks={'values': -1},\n",
    "                               decode_timedelta=False, )\n",
    "    lon = coords_only[\"longitude\"].compute().values\n",
    "    lon = lon_to_180(lon)\n",
    "    lat = coords_only[\"latitude\"].compute().values\n",
    "\n",
    "    if region_name == \"pan_arctic\":\n",
    "        geom = get_pan_arctic_shape()\n",
    "    else:\n",
    "        geom = get_region_shape(region_name)\n",
    "\n",
    "    pts = shapely.points(lon, lat)\n",
    "    mask_vals = shapely.contains(geom, pts)\n",
    "    true_count = int(np.count_nonzero(mask_vals))\n",
    "\n",
    "    mask_file = f'../data/processed/area_masks/{region_name}_mask.npy'\n",
    "    Path(mask_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(mask_file, mask_vals)\n",
    "    return mask_vals\n",
    "\n",
    "\n",
    "def load_and_apply_mask(dataset, region_name=\"pan_arctic\"):\n",
    "    \"\"\"\n",
    "    Loads a dataset, applies a mask, and returns the masked dataset. If the mask\n",
    "    does not already exist in the specified path, it is created and cached.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (xarray.Dataset): The dataset to which the mask will be applied.\n",
    "    region_name (str): The name of the region defining the mask. Defaults to\n",
    "        \"pan_arctic\".\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The dataset after applying the mask.\n",
    "\n",
    "    Raises:\n",
    "    FileNotFoundError: If the mask file cannot be created or found in the specified\n",
    "        path.\n",
    "    \"\"\"\n",
    "    mask_file = f'../data/processed/area_masks/{region_name}_mask.npy'\n",
    "\n",
    "    if not Path(mask_file).exists():\n",
    "        mask_vals = create_and_cache_mask(region_name)\n",
    "    else:\n",
    "        mask_vals = np.load(mask_file)\n",
    "\n",
    "    values_dim = dataset.sizes.get(\"values\", None)\n",
    "    time_dim = dataset.sizes.get(\"time\", None)\n",
    "    mask = xr.DataArray(mask_vals, dims=(\"values\",))\n",
    "    masked = dataset.where(mask, drop=True)\n",
    "    values_dim_after = masked.sizes.get(\"values\", None)\n",
    "    return masked\n",
    "\n",
    "\n",
    "def list_available_masks():\n",
    "    \"\"\"\n",
    "    Lists all available masks from the specified directory.\n",
    "\n",
    "    This function scans through a designated directory to identify files with\n",
    "    a specific naming pattern. It extracts the base names of the mask files,\n",
    "    removes unnecessary suffixes, and returns them in a sorted order. If the\n",
    "    designated directory is not found, an empty list is returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A sorted list of available mask names (excluding file suffixes and\n",
    "        unnecessary suffixes), or an empty list if the directory does not exist.\n",
    "    \"\"\"\n",
    "    mask_dir = Path('../data/processed/area_masks')\n",
    "    if mask_dir.exists():\n",
    "        masks = [f.stem.replace('_mask', '') for f in mask_dir.glob('*_mask.npy')]\n",
    "        masks_sorted = sorted(masks)\n",
    "        return masks_sorted\n",
    "    return []\n"
   ],
   "id": "7c481ec2b7542b88",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:55:17.019699Z",
     "start_time": "2025-08-28T10:55:14.789812Z"
    }
   },
   "cell_type": "code",
   "source": "client = Client(n_workers=2, threads_per_worker=6, memory_limit='8GB')",
   "id": "7e8eb0b98e867513",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:55:17.133379Z",
     "start_time": "2025-08-28T10:55:17.034734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "masked_dataset = load_and_apply_mask(selected_vars_dataset, region_name='Beaufort')\n",
    "masked_dataset = masked_dataset.persist()"
   ],
   "id": "a6eb85ad57b9849",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:57:38.265150Z",
     "start_time": "2025-08-28T10:57:38.063736Z"
    }
   },
   "cell_type": "code",
   "source": "masked_dataset.chunks",
   "id": "2cd7c130185d6dde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Frozen({'time': (720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 336), 'values': (1243,)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:57:30.201096Z",
     "start_time": "2025-08-28T10:55:19.573389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "daily_means = masked_dataset.resample(time='1D').mean()\n",
    "daily_means = daily_means.compute()\n",
    "df = daily_means.to_dataframe().reset_index()"
   ],
   "id": "5b8388856ab94cdf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\distributed\\client.py:3371: UserWarning: Sending large graph of size 19.36 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:52:04.979044Z",
     "start_time": "2025-08-28T10:52:03.450058Z"
    }
   },
   "cell_type": "code",
   "source": "client.close()",
   "id": "1e8ed070d3509e46",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
