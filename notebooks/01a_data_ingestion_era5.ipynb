{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "77c83102e976dfe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:02:44.354158Z",
     "start_time": "2025-08-21T15:02:44.321130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core scientific and data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.lib import unary_union\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n"
   ],
   "id": "799505498c75b301",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variables\n",
   "id": "ffa768fe592de812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:02:44.970170Z",
     "start_time": "2025-08-21T15:02:44.965604Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_url = 'gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr'",
   "id": "97df7811dd489449",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper Functions",
   "id": "3681e242326c231c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:02:45.594464Z",
     "start_time": "2025-08-21T15:02:45.588165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lon_to_360(x):\n",
    "    return (360 + (x % 360)) % 360\n",
    "\n",
    "def get_region_shape(region, shapefile = '../data/raw/shapefiles_regions/NSIDC-0780_SeaIceRegions_NH_v1.0.shp'):\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    region_of_interest = gdf[gdf['Region'] == region]\n",
    "\n",
    "    geom = unary_union(region_of_interest.geometry)\n",
    "\n",
    "    return geom.iloc[0]\n",
    "\n",
    "def get_pan_arctic_shape():\n",
    "    gdf = gpd.read_file('../data/raw/shapefiles_regions/NSIDC-0780_SeaIceRegions_NH_v1.0.shp').to_crs(\"EPSG:4326\")\n",
    "\n",
    "    name_col = \"Region\"\n",
    "    excl = {\"Baltic\",\"Japan\",\"Bohai\",\"Gulf_Alaska\",\"St_Lawr\",\"Okhotsk\"}\n",
    "    incl = [n for n in gdf[name_col] if n not in excl]\n",
    "\n",
    "    pan_arctic_geom = unary_union(gdf[gdf[name_col].isin(incl)].geometry)\n",
    "    return pan_arctic_geom.iloc[0]\n",
    "\n",
    "def slice_dataset_to_region(dataset, region):\n",
    "    if region == \"pan_arctic\":\n",
    "        geom = get_pan_arctic_shape()\n",
    "    else:\n",
    "        geom = get_region_shape(region)\n",
    "    lon = dataset[\"longitude\"].compute().values\n",
    "    lat = dataset[\"latitude\"].compute().values\n",
    "    pts = shapely.points(lon, lat)\n",
    "    mask_vals = shapely.contains(geom, pts)\n",
    "    mask = xr.DataArray(mask_vals, dims=(\"values\",))\n",
    "    return dataset.where(mask, drop=True)\n"
   ],
   "id": "62dba1bd394e3ea0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:02:47.363948Z",
     "start_time": "2025-08-21T15:02:46.043132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Opening the dataset\n",
    "reanalysis = xr.open_zarr(\n",
    "    dataset_url,\n",
    "    chunks={'time': 48, 'values': 'auto'},\n",
    "    consolidated=True,\n",
    "    decode_timedelta=False,)\n",
    "\n",
    "reanalysis = reanalysis.assign_coords(longitude=((reanalysis.longitude + 180) % 360) - 180)"
   ],
   "id": "3f197961012e1b7f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stein\\AppData\\Local\\Temp\\ipykernel_3012\\4168846803.py:2: FutureWarning: In a future version, xarray will not decode the variable 'step' into a timedelta64 dtype based on the presence of a timedelta-like 'units' attribute by default. Instead it will rely on the presence of a timedelta64 'dtype' attribute, which is now xarray's default way of encoding timedelta64 values.\n",
      "To continue decoding into a timedelta64 dtype, either set `decode_timedelta=True` when opening this dataset, or add the attribute `dtype='timedelta64[ns]'` to this variable on disk.\n",
      "To opt-in to future behavior, set `decode_timedelta=False`.\n",
      "  reanalysis = xr.open_zarr(\n",
      "C:\\Users\\stein\\AppData\\Local\\Temp\\ipykernel_3012\\4168846803.py:2: UserWarning: The specified chunks separate the stored chunks along dimension \"values\" starting at index 250000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  reanalysis = xr.open_zarr(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:02:47.387705Z",
     "start_time": "2025-08-21T15:02:47.382668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_and_cache_mask():\n",
    "    coords_only = xr.open_zarr(dataset_url, chunks={'values': -1},\n",
    "                               decode_timedelta=False,)\n",
    "    lon = coords_only[\"longitude\"].compute().values\n",
    "    lat = coords_only[\"latitude\"].compute().values\n",
    "\n",
    "    geom = get_pan_arctic_shape()\n",
    "    pts = shapely.points(lon, lat)\n",
    "    mask_vals = shapely.contains(geom, pts)\n",
    "\n",
    "    np.save('../data/processed/pan_arctic_mask.npy', mask_vals)\n",
    "    return mask_vals\n",
    "\n",
    "def load_and_apply_mask(dataset):\n",
    "    mask_vals = np.load('../data/processed/pan_arctic_mask.npy')\n",
    "    mask = xr.DataArray(mask_vals, dims=(\"values\",))\n",
    "    return dataset.where(mask, drop=True)"
   ],
   "id": "7c481ec2b7542b88",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:05:37.996650Z",
     "start_time": "2025-08-21T15:05:31.060183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "create_and_cache_mask()\n",
    "ds_roi = load_and_apply_mask(reanalysis)"
   ],
   "id": "d29e65450d6ea415",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stein\\AppData\\Local\\Temp\\ipykernel_3012\\624054296.py:2: FutureWarning: In a future version, xarray will not decode the variable 'step' into a timedelta64 dtype based on the presence of a timedelta-like 'units' attribute by default. Instead it will rely on the presence of a timedelta64 'dtype' attribute, which is now xarray's default way of encoding timedelta64 values.\n",
      "To continue decoding into a timedelta64 dtype, either set `decode_timedelta=True` when opening this dataset, or add the attribute `dtype='timedelta64[ns]'` to this variable on disk.\n",
      "To opt-in to future behavior, set `decode_timedelta=False`.\n",
      "  coords_only = xr.open_zarr(dataset_url, chunks={'values': -1})\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:05:53.566245Z",
     "start_time": "2025-08-21T15:05:50.703035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "ds_roi = ds_roi.chunk({'time': 1440, 'values': -1})\n",
    "\n",
    "lat = ds_roi[\"latitude\"]\n",
    "w = xr.DataArray(np.cos(np.deg2rad(lat)), dims=(\"values\",))\n"
   ],
   "id": "a04a590aafba4529",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:31:52.155875Z",
     "start_time": "2025-08-21T15:05:57.238519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_for_duplicates(year, base_dir=\"../data/processed\"):\n",
    "    outpath = Path(base_dir) / f\"pan_arctic_{year}.parquet\"\n",
    "    if outpath.exists():\n",
    "        print(f\"[skip ] {year} → {outpath} (already exists)\")\n",
    "        return True, str(outpath)\n",
    "    return False, str(outpath)\n",
    "\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=3, memory_limit=\"8GB\")\n",
    "\n",
    "years = range(1979, 2022)\n",
    "for y in years:\n",
    "    exists, outpath = check_for_duplicates(y)\n",
    "    if exists:\n",
    "        continue\n",
    "\n",
    "    print(f\"[start] {y}\")\n",
    "    t0 = time.perf_counter()\n",
    "    block = (ds_roi[[\"t2m\", \"u10\", \"v10\"]]\n",
    "             .sel(time=slice(f\"{y}-01-01\", f\"{y}-12-31\"))\n",
    "             .resample(time=\"1D\").mean()\n",
    "             .weighted(w).mean(\"values\"))\n",
    "\n",
    "    df_y = (block.assign(t2m_c=lambda d: d.t2m - 273.15)\n",
    "            .drop_vars(\"t2m\")\n",
    "            .to_dataframe()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"time\": \"date\"}))\n",
    "    df_y[\"region\"] = \"pan_arctic\"\n",
    "    df_y = df_y[[\"date\", \"region\", \"t2m_c\", \"u10\", \"v10\"]]\n",
    "    Path(outpath).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    table = pa.Table.from_pandas(df_y, preserve_index=False)\n",
    "    pq.write_table(table, outpath)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[done ] {y} → {outpath} ({dt:.1f}s)\")\n"
   ],
   "id": "eb6cb8e9be5f50d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 64961 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip ] 1979 → ..\\data\\processed\\pan_arctic_1979.parquet (already exists)\n",
      "[skip ] 1980 → ..\\data\\processed\\pan_arctic_1980.parquet (already exists)\n",
      "[skip ] 1981 → ..\\data\\processed\\pan_arctic_1981.parquet (already exists)\n",
      "[skip ] 1982 → ..\\data\\processed\\pan_arctic_1982.parquet (already exists)\n",
      "[skip ] 1983 → ..\\data\\processed\\pan_arctic_1983.parquet (already exists)\n",
      "[skip ] 1984 → ..\\data\\processed\\pan_arctic_1984.parquet (already exists)\n",
      "[skip ] 1985 → ..\\data\\processed\\pan_arctic_1985.parquet (already exists)\n",
      "[skip ] 1986 → ..\\data\\processed\\pan_arctic_1986.parquet (already exists)\n",
      "[skip ] 1987 → ..\\data\\processed\\pan_arctic_1987.parquet (already exists)\n",
      "[skip ] 1988 → ..\\data\\processed\\pan_arctic_1988.parquet (already exists)\n",
      "[skip ] 1989 → ..\\data\\processed\\pan_arctic_1989.parquet (already exists)\n",
      "[skip ] 1990 → ..\\data\\processed\\pan_arctic_1990.parquet (already exists)\n",
      "[skip ] 1991 → ..\\data\\processed\\pan_arctic_1991.parquet (already exists)\n",
      "[skip ] 1992 → ..\\data\\processed\\pan_arctic_1992.parquet (already exists)\n",
      "[skip ] 1993 → ..\\data\\processed\\pan_arctic_1993.parquet (already exists)\n",
      "[skip ] 1994 → ..\\data\\processed\\pan_arctic_1994.parquet (already exists)\n",
      "[skip ] 1995 → ..\\data\\processed\\pan_arctic_1995.parquet (already exists)\n",
      "[skip ] 1996 → ..\\data\\processed\\pan_arctic_1996.parquet (already exists)\n",
      "[skip ] 1997 → ..\\data\\processed\\pan_arctic_1997.parquet (already exists)\n",
      "[skip ] 1998 → ..\\data\\processed\\pan_arctic_1998.parquet (already exists)\n",
      "[skip ] 1999 → ..\\data\\processed\\pan_arctic_1999.parquet (already exists)\n",
      "[skip ] 2000 → ..\\data\\processed\\pan_arctic_2000.parquet (already exists)\n",
      "[skip ] 2001 → ..\\data\\processed\\pan_arctic_2001.parquet (already exists)\n",
      "[skip ] 2002 → ..\\data\\processed\\pan_arctic_2002.parquet (already exists)\n",
      "[skip ] 2003 → ..\\data\\processed\\pan_arctic_2003.parquet (already exists)\n",
      "[skip ] 2004 → ..\\data\\processed\\pan_arctic_2004.parquet (already exists)\n",
      "[skip ] 2005 → ..\\data\\processed\\pan_arctic_2005.parquet (already exists)\n",
      "[skip ] 2006 → ..\\data\\processed\\pan_arctic_2006.parquet (already exists)\n",
      "[skip ] 2007 → ..\\data\\processed\\pan_arctic_2007.parquet (already exists)\n",
      "[skip ] 2008 → ..\\data\\processed\\pan_arctic_2008.parquet (already exists)\n",
      "[start] 2009\n",
      "[done ] 2009 → ..\\data\\processed\\pan_arctic_2009.parquet (1525.1s)\n",
      "[start] 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 17:31:51,523 - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-2255634' coro=<Client._gather.<locals>.wait() done, defined at C:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\distributed\\client.py:2385> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\distributed\\client.py\", line 2394, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     18\u001B[39m t0 = time.perf_counter()\n\u001B[32m     19\u001B[39m block = (ds_roi[[\u001B[33m\"\u001B[39m\u001B[33mt2m\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mu10\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mv10\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m     20\u001B[39m          .sel(time=\u001B[38;5;28mslice\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-01-01\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-12-31\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m     21\u001B[39m          .resample(time=\u001B[33m\"\u001B[39m\u001B[33m1D\u001B[39m\u001B[33m\"\u001B[39m).mean()\n\u001B[32m     22\u001B[39m          .weighted(w).mean(\u001B[33m\"\u001B[39m\u001B[33mvalues\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m     24\u001B[39m df_y = (\u001B[43mblock\u001B[49m\u001B[43m.\u001B[49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt2m_c\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m.\u001B[49m\u001B[43mt2m\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m273.15\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m        \u001B[49m\u001B[43m.\u001B[49m\u001B[43mdrop_vars\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mt2m\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[43m        \u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m         .reset_index()\n\u001B[32m     28\u001B[39m         .rename(columns={\u001B[33m\"\u001B[39m\u001B[33mtime\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m\"\u001B[39m}))\n\u001B[32m     29\u001B[39m df_y[\u001B[33m\"\u001B[39m\u001B[33mregion\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mpan_arctic\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     30\u001B[39m df_y = df_y[[\u001B[33m\"\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mregion\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mt2m_c\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mu10\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mv10\u001B[39m\u001B[33m\"\u001B[39m]]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\xarray\\core\\dataset.py:7273\u001B[39m, in \u001B[36mDataset.to_dataframe\u001B[39m\u001B[34m(self, dim_order)\u001B[39m\n\u001B[32m   7245\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Convert this dataset into a pandas.DataFrame.\u001B[39;00m\n\u001B[32m   7246\u001B[39m \n\u001B[32m   7247\u001B[39m \u001B[33;03mNon-index variables in this dataset form the columns of the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   7268\u001B[39m \n\u001B[32m   7269\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   7271\u001B[39m ordered_dims = \u001B[38;5;28mself\u001B[39m._normalize_dim_order(dim_order=dim_order)\n\u001B[32m-> \u001B[39m\u001B[32m7273\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_to_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mordered_dims\u001B[49m\u001B[43m=\u001B[49m\u001B[43mordered_dims\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\xarray\\core\\dataset.py:7213\u001B[39m, in \u001B[36mDataset._to_dataframe\u001B[39m\u001B[34m(self, ordered_dims)\u001B[39m\n\u001B[32m   7202\u001B[39m extension_array_columns_different_index = [\n\u001B[32m   7203\u001B[39m     k\n\u001B[32m   7204\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m extension_array_columns\n\u001B[32m   7205\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m.variables[k].dims) != \u001B[38;5;28mset\u001B[39m(ordered_dims.keys())\n\u001B[32m   7206\u001B[39m ]\n\u001B[32m   7207\u001B[39m extension_array_columns_same_index = [\n\u001B[32m   7208\u001B[39m     k\n\u001B[32m   7209\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m extension_array_columns\n\u001B[32m   7210\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m extension_array_columns_different_index\n\u001B[32m   7211\u001B[39m ]\n\u001B[32m   7212\u001B[39m data = [\n\u001B[32m-> \u001B[39m\u001B[32m7213\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_variables\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mset_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mordered_dims\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m.reshape(-\u001B[32m1\u001B[39m)\n\u001B[32m   7214\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m non_extension_array_columns\n\u001B[32m   7215\u001B[39m ]\n\u001B[32m   7216\u001B[39m index = \u001B[38;5;28mself\u001B[39m.coords.to_index([*ordered_dims])\n\u001B[32m   7217\u001B[39m broadcasted_df = pd.DataFrame(\n\u001B[32m   7218\u001B[39m     {\n\u001B[32m   7219\u001B[39m         **\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(non_extension_array_columns, data, strict=\u001B[38;5;28;01mTrue\u001B[39;00m)),\n\u001B[32m   (...)\u001B[39m\u001B[32m   7225\u001B[39m     index=index,\n\u001B[32m   7226\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\xarray\\core\\variable.py:556\u001B[39m, in \u001B[36mVariable.values\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    554\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mvalues\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> np.ndarray:\n\u001B[32m    555\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_as_array_or_item\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\xarray\\core\\variable.py:336\u001B[39m, in \u001B[36m_as_array_or_item\u001B[39m\u001B[34m(data)\u001B[39m\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_as_array_or_item\u001B[39m(data):\n\u001B[32m    323\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001B[39;00m\n\u001B[32m    324\u001B[39m \u001B[33;03m    it's a 0d datetime64 or timedelta64 array.\u001B[39;00m\n\u001B[32m    325\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    334\u001B[39m \u001B[33;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001B[39;00m\n\u001B[32m    335\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m     data = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data.ndim == \u001B[32m0\u001B[39m:\n\u001B[32m    338\u001B[39m         kind = data.dtype.kind\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\dask\\array\\core.py:1729\u001B[39m, in \u001B[36mArray.__array__\u001B[39m\u001B[34m(self, dtype, copy, **kwargs)\u001B[39m\n\u001B[32m   1722\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m   1723\u001B[39m     warnings.warn(\n\u001B[32m   1724\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCan\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt acquire a memory view of a Dask array. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1725\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThis will raise in the future.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1726\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m   1727\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1729\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[38;5;66;03m# Apply requested dtype and convert non-numpy backends to numpy.\u001B[39;00m\n\u001B[32m   1732\u001B[39m \u001B[38;5;66;03m# If copy is True, numpy is going to perform its own deep copy\u001B[39;00m\n\u001B[32m   1733\u001B[39m \u001B[38;5;66;03m# after this method returns.\u001B[39;00m\n\u001B[32m   1734\u001B[39m \u001B[38;5;66;03m# If copy is None, finalize() ensures that the returned object\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;66;03m# does not share memory with an object stored in the graph or on a\u001B[39;00m\n\u001B[32m   1736\u001B[39m \u001B[38;5;66;03m# process-local Worker.\u001B[39;00m\n\u001B[32m   1737\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.asarray(x, dtype=dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\dask\\base.py:373\u001B[39m, in \u001B[36mDaskMethodsMixin.compute\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    349\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, **kwargs):\n\u001B[32m    350\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Compute this dask collection\u001B[39;00m\n\u001B[32m    351\u001B[39m \n\u001B[32m    352\u001B[39m \u001B[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    371\u001B[39m \u001B[33;03m    dask.compute\u001B[39;00m\n\u001B[32m    372\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m     (result,) = \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraverse\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    374\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\projects\\private_projects\\arctic-ice-extent\\.venv\\Lib\\site-packages\\dask\\base.py:681\u001B[39m, in \u001B[36mcompute\u001B[39m\u001B[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001B[39m\n\u001B[32m    678\u001B[39m     expr = expr.optimize()\n\u001B[32m    679\u001B[39m     keys = \u001B[38;5;28mlist\u001B[39m(flatten(expr.__dask_keys__()))\n\u001B[32m--> \u001B[39m\u001B[32m681\u001B[39m     results = \u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    683\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m repack(results)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    653\u001B[39m signaled = \u001B[38;5;28mself\u001B[39m._flag\n\u001B[32m    654\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m     signaled = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cond\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m359\u001B[39m         gotit = \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    360\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    361\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "client.close()\n",
   "id": "831bf67c7c06e4b1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
