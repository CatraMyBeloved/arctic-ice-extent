{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "77c83102e976dfe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:46:40.193230Z",
     "start_time": "2025-09-14T12:46:37.120878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core scientific and data libraries\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from src.coordinate_utils import *\n",
    "from dask.distributed import Client"
   ],
   "id": "799505498c75b301",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variables\n",
   "id": "ffa768fe592de812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:46:44.602362Z",
     "start_time": "2025-09-14T12:46:44.598935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_url = 'gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr'\n",
    "relevant_regions = ['Central_Arctic', 'Beaufort', 'Chukchi-NA', 'Chukchi-Asia', 'E_Siberian', 'Laptev', 'Kara', 'Barents', 'E_Greenland', 'Baffin', 'Hudson', 'Can_Arch', 'Bering-NA', 'Bering-Asia', 'pan_arctic']"
   ],
   "id": "97df7811dd489449",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.287658Z",
     "start_time": "2025-09-14T12:46:44.603366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = xr.open_zarr(\n",
    "    dataset_url,\n",
    "    consolidated=True,\n",
    "    decode_timedelta=False,\n",
    "    chunks={'time': 240},\n",
    ")\n",
    "dataset = dataset.assign_coords(longitude=((dataset.longitude + 180) % 360) - 180)\n"
   ],
   "id": "2e6b638e550628c3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.292296Z",
     "start_time": "2025-09-14T12:47:07.288222Z"
    }
   },
   "cell_type": "code",
   "source": "selected_vars_dataset = dataset[[\"t2m\", \"u10\", \"v10\", \"msl\"]]",
   "id": "7f9cb90250ef6c6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper Functions",
   "id": "3681e242326c231c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.304722Z",
     "start_time": "2025-09-14T12:47:07.296301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_and_cache_mask(region_name):\n",
    "    \"\"\"\n",
    "    Creates and caches a mask for a specified geographical region.\n",
    "\n",
    "    This function generates a mask by determining whether geographical points\n",
    "    belong to a specified region shape, using longitude and latitude coordinates\n",
    "    from a dataset. The mask is then cached as a .npy file for future use.\n",
    "    It supports both \"pan_arctic\" and other custom region names.\n",
    "\n",
    "    Parameters:\n",
    "        region_name (str): Name of the geographical region for which the mask\n",
    "            is to be created. If \"pan_arctic\", a predefined shape is used, otherwise\n",
    "            a specified regional shape is retrieved.\n",
    "\n",
    "    Raises:\n",
    "        Any exceptions associated with file I/O or operations on the dataset.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A boolean array where each element indicates whether\n",
    "            the corresponding geographical point is within the specified region.\n",
    "    \"\"\"\n",
    "    coords_only = xr.open_zarr(dataset_url, chunks={'values': -1},\n",
    "                               decode_timedelta=False, )\n",
    "    lon = coords_only[\"longitude\"].compute().values\n",
    "    lon = lon_to_180(lon)\n",
    "    lat = coords_only[\"latitude\"].compute().values\n",
    "\n",
    "    if region_name == \"pan_arctic\":\n",
    "        geom = get_pan_arctic_shape()\n",
    "    else:\n",
    "        geom = get_region_shape(region_name)\n",
    "\n",
    "    pts = shapely.points(lon, lat)\n",
    "    mask_vals = shapely.contains(geom, pts)\n",
    "\n",
    "    mask_file = f'../data/processed/area_masks/{region_name}_mask.npy'\n",
    "    Path(mask_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(mask_file, mask_vals)\n",
    "    return mask_vals\n",
    "\n",
    "\n",
    "def load_and_apply_mask(dataset, region_name=\"pan_arctic\"):\n",
    "    \"\"\"\n",
    "    Loads a dataset, applies a mask, and returns the masked dataset. If the mask\n",
    "    does not already exist in the specified path, it is created and cached.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (xarray.Dataset): The dataset to which the mask will be applied.\n",
    "    region_name (str): The name of the region defining the mask. Defaults to\n",
    "        \"pan_arctic\".\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The dataset after applying the mask.\n",
    "\n",
    "    Raises:\n",
    "    FileNotFoundError: If the mask file cannot be created or found in the specified\n",
    "        path.\n",
    "    \"\"\"\n",
    "    mask_file = f'../data/processed/area_masks/{region_name}_mask.npy'\n",
    "\n",
    "    if not Path(mask_file).exists():\n",
    "        mask_vals = create_and_cache_mask(region_name)\n",
    "    else:\n",
    "        mask_vals = np.load(mask_file)\n",
    "\n",
    "    values_dim = dataset.sizes.get(\"values\", None)\n",
    "    time_dim = dataset.sizes.get(\"time\", None)\n",
    "    mask = xr.DataArray(mask_vals, dims=(\"values\",))\n",
    "    masked = dataset.where(mask, drop=True)\n",
    "    values_dim_after = masked.sizes.get(\"values\", None)\n",
    "    return masked\n",
    "\n",
    "\n",
    "def list_available_masks():\n",
    "    \"\"\"\n",
    "    Lists all available masks from the specified directory.\n",
    "\n",
    "    This function scans through a designated directory to identify files with\n",
    "    a specific naming pattern. It extracts the base names of the mask files,\n",
    "    removes unnecessary suffixes, and returns them in a sorted order. If the\n",
    "    designated directory is not found, an empty list is returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A sorted list of available mask names (excluding file suffixes and\n",
    "        unnecessary suffixes), or an empty list if the directory does not exist.\n",
    "    \"\"\"\n",
    "    mask_dir = Path('../data/processed/area_masks')\n",
    "    if mask_dir.exists():\n",
    "        masks = [f.stem.replace('_mask', '') for f in mask_dir.glob('*_mask.npy')]\n",
    "        masks_sorted = sorted(masks)\n",
    "        return masks_sorted\n",
    "    return []\n",
    "\n",
    "def daily_stats_all_vars(region_ds, year, variables):\n",
    "    \"\"\"\n",
    "    region_ds: masked + persisted dataset containing *all* variables\n",
    "    returns: pandas DataFrame with daily mean/std/p15/p85 for each variable\n",
    "    \"\"\"\n",
    "    # 1) daily temporal mean (still per-gridcell)\n",
    "    daily = region_ds[variables].sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\")) \\\n",
    "                                .resample(time=\"1D\").mean()\n",
    "\n",
    "    # 2) spatial reductions stay in Dask\n",
    "    mu  = daily.mean(dim=\"values\")                    # mean over grid\n",
    "    sd  = daily.std(dim=\"values\")                     # std over grid\n",
    "    q   = daily.quantile([0.15, 0.85], dim=\"values\")  # quantiles over grid  (lazy)\n",
    "\n",
    "    # 3) assemble compact Dataset (dims ~ time only)\n",
    "    out = xr.Dataset()\n",
    "    for var in variables:\n",
    "        out[f\"{var}_mean\"] = mu[var]\n",
    "        out[f\"{var}_std\"]  = sd[var]\n",
    "        out[f\"{var}_p15\"]  = q.sel(quantile=0.15)[var]\n",
    "        out[f\"{var}_p85\"]  = q.sel(quantile=0.85)[var]\n",
    "\n",
    "    # 4) small df now; compute happens here\n",
    "    df = out.to_dataframe().reset_index()\n",
    "    return df\n"
   ],
   "id": "7c481ec2b7542b88",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8b1lye426pj",
   "source": "#### Systematic Regional-Yearly Processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "otm6a5fxvca",
   "source": "def get_parquet_path(region, year, variable):\n    \"\"\"\n    Generates the file path for parquet files following the project structure.\n    \n    Parameters:\n        region (str): Name of the geographical region\n        year (int): Year for the data\n        variable (str): ERA5 variable name (t2m, u10, v10, msl)\n    \n    Returns:\n        Path: Path object for the parquet file\n    \"\"\"\n    base_dir = Path('../data/processed/parquet/year')\n    filename = f\"{region}_{year}_{variable}.parquet\"\n    return base_dir / filename",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.312724Z",
     "start_time": "2025-09-14T12:47:07.305732Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "pul7smjzsqe",
   "source": "def calculate_daily_statistics(dataset, region, year, variable):\n    \"\"\"\n    Calculates daily statistics for a specific region, year, and variable.\n    \n    Parameters:\n        dataset (xarray.Dataset): The ERA5 dataset\n        region (str): Name of the geographical region\n        year (int): Year for the data processing\n        variable (str): ERA5 variable name\n    \n    Returns:\n        pandas.DataFrame: DataFrame with daily statistics\n    \"\"\"\n    # Apply regional mask\n    masked_dataset = load_and_apply_mask(dataset[[variable]], region_name=region)\n    masked_dataset = masked_dataset.persist()\n    \n    # Select year and resample to daily means\n    year_data = masked_dataset.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n    daily_means = year_data.resample(time='1D').mean().compute()\n    \n    # Convert to DataFrame for easier statistics calculation\n    df = daily_means.to_dataframe().reset_index()\n    \n    # Calculate daily statistics\n    daily_stats = df.groupby(df['time'].dt.date).agg({\n        variable: ['mean', 'std', lambda x: np.percentile(x, 15), lambda x: np.percentile(x, 85)]\n    }).round(4)\n    \n    # Flatten column names\n    daily_stats.columns = [f'{variable}_mean', f'{variable}_std', f'{variable}_p15', f'{variable}_p85']\n    daily_stats = daily_stats.reset_index()\n    daily_stats['region'] = region\n    daily_stats['year'] = year\n    \n    return daily_stats",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.321436Z",
     "start_time": "2025-09-14T12:47:07.313733Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "6d4xa9e7hx9",
   "source": [
    "def process_regions_and_years(dataset, regions, years, variables):\n",
    "    total_combinations = len(regions) * len(years) * len(variables)\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    print(f\"Processing {total_combinations} region-year-variable combinations...\")\n",
    "\n",
    "    for region in regions:\n",
    "        print(f\"\\nProcessing region: {region}\")\n",
    "\n",
    "        # Mask once per region and persist for reuse across all years\n",
    "        region_ds = load_and_apply_mask(dataset[variables], region_name=region)\n",
    "        region_ds = region_ds.persist()\n",
    "\n",
    "        for year in years:\n",
    "            # If *all* var files for this (region,year) exist, skip quickly\n",
    "            if all(get_parquet_path(region, year, v).exists() for v in variables):\n",
    "                skipped_count += len(variables)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats_df_all = daily_stats_all_vars(region_ds, year, variables)\n",
    "                stats_df_all[\"region\"] = region\n",
    "                stats_df_all[\"year\"] = year\n",
    "\n",
    "                # write one file per variable (keeps your existing structure)\n",
    "                for v in variables:\n",
    "                    parquet_path = get_parquet_path(region, year, v)\n",
    "                    if parquet_path.exists():\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                    parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    cols = [\"time\", \"region\", \"year\",\n",
    "                            f\"{v}_mean\", f\"{v}_std\", f\"{v}_p15\", f\"{v}_p85\"]\n",
    "                    # Rename time→date if you prefer that schema\n",
    "                    out = stats_df_all[[\"time\", \"region\", \"year\",\n",
    "                                        f\"{v}_mean\", f\"{v}_std\", f\"{v}_p15\", f\"{v}_p85\"]].copy()\n",
    "                    out.rename(columns={\"time\": \"date\"}, inplace=True)\n",
    "                    out.to_parquet(parquet_path, index=False)\n",
    "                    processed_count += 1\n",
    "                    print(f\"  ✓ {parquet_path.name} created\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {region}_{year}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Files processed: {processed_count}\")\n",
    "    print(f\"Files skipped (already exist): {skipped_count}\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.330460Z",
     "start_time": "2025-09-14T12:47:07.322452Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "qmbr1f8udpq",
   "source": "# Define processing parameters\nyears_to_process = list(range(1979, 2022))  # 1979 to 2021 inclusive\nvariables_to_process = [\"t2m\", \"u10\", \"v10\", \"msl\"]\n\nprint(f\"Total years: {len(years_to_process)} (1979-2021)\")\nprint(f\"Total regions: {len(relevant_regions)}\")\nprint(f\"Total variables: {len(variables_to_process)}\")\nprint(f\"Total combinations: {len(relevant_regions) * len(years_to_process) * len(variables_to_process)}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:07.338809Z",
     "start_time": "2025-09-14T12:47:07.330460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total years: 43 (1979-2021)\n",
      "Total regions: 15\n",
      "Total variables: 4\n",
      "Total combinations: 2580\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "pnsyeeyzdb",
   "source": "# Test with a small subset first\ntest_regions = ['Beaufort']  # Start with one region\ntest_years = [2019, 2020]    # Test with two years\ntest_variables = ['t2m']     # Test with one variable\n\nprint(\"Starting test run with subset...\")\nprint(f\"Test regions: {test_regions}\")\nprint(f\"Test years: {test_years}\")  \nprint(f\"Test variables: {test_variables}\")\n\n# Restart dask client for clean processing\nclient = Client(n_workers=4, threads_per_worker=3, memory_limit='8GB')",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:47:09.945516Z",
     "start_time": "2025-09-14T12:47:07.340198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test run with subset...\n",
      "Test regions: ['Beaufort']\n",
      "Test years: [2019, 2020]\n",
      "Test variables: ['t2m']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "uudzssmq63",
   "source": [
    "process_regions_and_years(\n",
    "    dataset=selected_vars_dataset,\n",
    "    regions=relevant_regions,\n",
    "    years=years_to_process,\n",
    "    variables=variables_to_process)\n"
   ],
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-14T12:47:09.946606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2580 region-year-variable combinations...\n",
      "\n",
      "Processing region: Central_Arctic\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab27cde26b8f8978",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
